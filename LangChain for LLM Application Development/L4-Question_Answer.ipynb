{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use language models and combine it with a lot of our documents. But there's a key issue. \n",
    "Language models can only inspect a few thousand words at a time. So if we have really large documents, how can we get the language model to answer questions about everything that's in there? \n",
    "\n",
    "This is where embeddings and vector stores come into play. <br>\n",
    "First, let's talk about embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "- embedding vector captures content/meaning. \n",
    "- Text with similar content will have similar vectors. \n",
    "Embeddings are useful as we think about which pieces of text we want to include when \n",
    "passing them to the language model to answer a question. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector database\n",
    "\n",
    "The next component that we're going to cover is the vector database. \n",
    "A vector database is a way to store these vector representations that we created in the previous step. \n",
    "\n",
    "It is insufficient to store the whole vector representations of incoming documents. Instead, we split each incoming document into chunks, embed each chunk, index each chunk via locality sensitivity hasing (LHS) or Product Quantization (PQ) etc, and store the index representation and the index lookup table (which maps given index to the corresponding embedding vector). \n",
    "\n",
    "By doing so, we blur each document's original embedding to a rough embedding representation. Though it may not be 100% accurate, but it saves memory. Furthermore, since now the number of embeddings are limited, we can represent them via a index and use a index table to do the lookup when needed, which further compress memory usage. \n",
    "\n",
    "Reference: [What is a Vector Database?](https://www.pinecone.io/learn/vector-database/)\n",
    "\n",
    "\n",
    "When we get a big incoming document, we're first going to break it \n",
    "up into smaller chunks. \n",
    "This helps create pieces of text that are \n",
    "smaller than the original document, which is useful because \n",
    "we may not be able to pass the whole document to the \n",
    "language model.\n",
    "So we want to create these small chunks \n",
    "so we can only pass the most relevant \n",
    "ones to the language model. \n",
    "We then create an embedding for each of these chunks, \n",
    "and then we store those in a vector database. That's \n",
    "what happens when we create the index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pip install --upgrade langchain\n",
    "#pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd \n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "\n",
    "df = pd.read_csv(file, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])\n",
    "\n",
    "query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\"\n",
    "response = index.query(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=file)\n",
    "docs = loader.load()\n",
    "docs[0] # each index/doc is a row in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# query/sentence embedding\n",
    "embed = embeddings.embed_query(\"Hi my name is Harrison\")\n",
    "print(len(embed))\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector database from docs and a embedding model\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can query on db\n",
    "query = \"Please suggest a shirt with sunblocking\"\n",
    "docs = db.similarity_search(query)\n",
    "# it returns 4 similar docs based on query\n",
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from vector database, which is a generic interface \n",
    "# that can be underpinned by any method that takes in queries and outputs docs\n",
    "# It's an interface for fetching docs\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0.0)\n",
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
    "print(qdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
    "shirts with sun protection in a table in markdown and summarize each one.\") \n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\"\n",
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above are the same as the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])\n",
    "\n",
    "response = index.query(query, llm=llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
